{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz2aYwDao1Yk"
   },
   "source": [
    "## 토픽 모델링\n",
    "* 실습을 위해 pyLDAvis 설치가 필요합니다. \n",
    "* colab사용시 설치 후에도 제대로 동작하지 않거나 오류가 나면 런타임을 재실행 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcdv-HRKo1Yp"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI_S8brso1Yr"
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kpwaWyzo1Yr"
   },
   "source": [
    "## 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jEmmKL0o1Ys"
   },
   "outputs": [],
   "source": [
    "# 필요 라이브러리를 로드합니다.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MzjjX3ho1Ys"
   },
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZd0hR-Vo1Yu",
    "outputId": "81941984-21f5-40b9-a797-e3a4711bf656"
   },
   "outputs": [],
   "source": [
    "# 수집한 데이터셋을 불러옵니다.\n",
    "df = pd.read_csv(\"https://bit.ly/seoul-120-text-csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치가 있다면 제거합니다.\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXAtNDKco1Yv",
    "outputId": "efc483df-cb02-4fe7-fdf1-8a30a9d8a382",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 결측치를 확인합니다.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw4p_dnIo1Yw"
   },
   "source": [
    "## 문서 만들기\n",
    "* 제목과 내용을 함께 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ9GAlhuo1Yx"
   },
   "outputs": [],
   "source": [
    "df[\"문서\"] = df[\"제목\"] + \" \" + df[\"내용\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMII7R0go1Yx"
   },
   "source": [
    "## 벡터화\n",
    "\n",
    "* [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "\n",
    "## CountVectorizer\n",
    "\n",
    "* analyzer : 단어, 문자 단위의 벡터화 방법 정의\n",
    "* ngram_range : BOW 단위 수 (1, 3) 이라면 1개~3개까지 토큰을 묶어서 벡터화\n",
    "* max_df : 어휘를 작성할 때 문서 빈도가 주어진 임계값보다 높은 용어(말뭉치 관련 불용어)는 제외 (기본값=1.0)\n",
    "    * max_df = 0.90 : 문서의 90% 이상에 나타나는 단어 제외\n",
    "    * max_df = 10 : 10개 이상의 문서에 나타나는 단어 제외\n",
    "* min_df : 어휘를 작성할 때 문서 빈도가 주어진 임계값보다 낮은 용어는 제외합니다. 컷오프라고도 합니다.(기본값=1.0)\n",
    "    * min_df = 0.01 : 문서의 1% 미만으로 나타나는 단어 제외\n",
    "    * min_df = 10 : 문서에 10개 미만으로 나타나는 단어 제외\n",
    "* stop_words : 불용어 정의\n",
    "* API Document: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94FBGEqOo1Yy",
    "outputId": "805f1a7a-4bfd-4893-8efb-fa8adec881db"
   },
   "outputs": [],
   "source": [
    "# 단어들의 카운트(출현 빈도(frequency))로 여러 문서들을 벡터화하기 위해 CountVectorizer를 불러옵니다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(stop_words=[\"돋움\", \"경우\", \"또는\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft2MvWD57MjT"
   },
   "source": [
    "### 참고: fit, transform, fit_transfrom의 차이점\n",
    "- fit(): 원시 문서에 있는 모든 토큰의 어휘 사전을 배운다\n",
    "- transform(): 문서를 문서 용어 매트릭스로 변환, transform 이후엔 매트릭스로 변환되어 숫자형태로 변경\n",
    "- fit_transform(): 어휘 사전을 배우고 문서 용어 매트릭스를 반환, fit 다음에 변환이 오는 것과 동일하지만 더 효율적으로 구현\n",
    "\n",
    "* API Document: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YueqXKDbo1Yy",
    "outputId": "08eb5692-c065-4de2-81f6-b2bb888012ba"
   },
   "outputs": [],
   "source": [
    "# fit_transform을 사용하여 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 변수 Document Term Matrix(이하 dtm)를 생성합니다.\n",
    "dtm_cv = cv.fit_transform(df[\"문서\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr_Gj0Bro1Yy",
    "outputId": "414c044e-afea-4e3a-f211-28949b95b8fa"
   },
   "outputs": [],
   "source": [
    "# cv.vocabulary_ 를 봅니다.\n",
    "# cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_cols = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6sZ_cIdo1Yz",
    "outputId": "3cc8986f-c25c-4f26-c303-971e491f793b"
   },
   "outputs": [],
   "source": [
    "# 각 row에서 전체 단어가방에 있는 어휘에서 등장하는 단어에 대한 one-hot-vector를 확인합니다.\n",
    "# toarray()로 희소 행렬(sparse matrix, 행렬의 값이 대부분 '0'인 행렬)을 NumPy array 배열로 변환하여 값을 확인합니다.\n",
    "\n",
    "pd.DataFrame(dtm_cv.toarray(), columns=cv_cols).sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZKMAZ5go1Yz"
   },
   "source": [
    "## BOW<sup>bag of word</sup> 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)\n",
    "\n",
    "* API documentation: https://pyldavis.readthedocs.io/en/latest/modules/API.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj6AiX1Po1Y0",
    "outputId": "792e1658-3d33-4ab0-f777-ff8e698b7d32"
   },
   "outputs": [],
   "source": [
    "# 정답인 '분류'의 유일한 값을 확인하여 주제 수를 확인합니다.\n",
    "df[\"분류\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQBuD_fJo1Y0",
    "outputId": "08a96ed7-5e37-4534-afbb-efa6077a657d"
   },
   "outputs": [],
   "source": [
    "# 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지를 확인하는 잠재 디리클레 분석(LDA)을 불러옵니다.\n",
    "# n_components에 넣을 하이퍼파라미터 NUM_TOPICS로 주제수를 설정합니다.(기본값=10)\n",
    "# max_iter는 훈련 데이터(epoch라고도 함)에 대한 최대 패스 수입니다.(기본값=10)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "NUM_TOPICS = 10\n",
    "LDA_model = LatentDirichletAllocation(n_components=NUM_TOPICS, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQBuD_fJo1Y0",
    "outputId": "08a96ed7-5e37-4534-afbb-efa6077a657d"
   },
   "outputs": [],
   "source": [
    "# LDA_model 에 dtm_cv 를 넣어 학습합니다.\n",
    "LDA_model.fit(dtm_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sbyb39Szo1Y0",
    "outputId": "8f858b31-2a81-4b8f-aa5f-45411fc902c8"
   },
   "outputs": [],
   "source": [
    "# 토픽 모델링에 이용되는 LDA 모델의 학습 결과를 시각화하는 Python 라이브러리인 pyLDAvis를 불러옵니다.\n",
    "# mds(Multi-Dimensional Scaling)는 데이터 포인트 간의 거리를 보존하면서 차원을 축소하는 기법입니다.\n",
    "# t-SNE(t-Stochastic Neighbor Embedding)은 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용합니다.\n",
    "\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(LDA_model, dtm_cv, cv, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bduBjLQZo1Y1"
   },
   "source": [
    "## TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "## TfidfVectorizer\n",
    "\n",
    "TF-IDF 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어(낮은 구별력)의 경우 가중치를 축소하는 방법\n",
    "\n",
    "매개변수\n",
    "* norm='l2' 각 문서의 피처 벡터를 어떻게 벡터 정규화 할지 정한다. \n",
    "    - L2 : 벡터의 각 원소의 제곱의 합이 1이 되도록 만드는 것이고 기본 값\n",
    "    - L1 : 벡터의 각 원소의 절댓값의 합이 1이 되도록 크기를 조절\n",
    "* smooth_idf=False\n",
    "    - 피처를 만들 때 0으로 나오는 항목에 대해 작은 값을 더해서(스무딩을 해서) 피처를 만들지 아니면 그냥 생성할지를 결정\n",
    "* sublinear_tf=False\n",
    "* use_idf=True\n",
    "    - TF-IDF를 사용해 피처를 만들 것인지 아니면 단어 빈도 자체를 사용할 것인지 여부\n",
    "* API Document: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEUNepLOo1Y1",
    "outputId": "7d39e765-68ad-437e-d37c-14a0b03de823"
   },
   "outputs": [],
   "source": [
    "# TF-IDF 방식으로 단어의 가중치를 조정한 BOW 인코딩하여 벡터화하기 위해 TfidfVectorizer를 불러옵니다.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=[\"돋움\", \"경우\", \"또는\", \"있습니다\", \"있는\", \"합니다\"])\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKEXeuKQo1Y1",
    "outputId": "157f2a5c-9597-45c3-de7a-c9ef1cc1df8e"
   },
   "outputs": [],
   "source": [
    "# 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 변수 Document Term Matrix(이하 dtm)를 생성합니다.\n",
    "dtm_tfidf = tfidf.fit_transform(df[\"문서\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLkWfsggo1Y1",
    "outputId": "9256c021-cdb8-4173-d1f6-440f975073de"
   },
   "outputs": [],
   "source": [
    "# tfidf.vocabulary_ 의 번호는 정렬 순으로 되어 있습니다.\n",
    "# tfidf.vocabulary_\n",
    "cols_tfidf = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8W95ppMo1Y2",
    "outputId": "76547c25-abeb-4190-ca0e-c49359cd1858"
   },
   "outputs": [],
   "source": [
    "# dtm_tf를 axis=0(수직 방향으로) 기준으로 합계를 낸 dist 변수를 생성합니다.\n",
    "# dist 변수를 vocabulary_ 순으로 정렬하여 비율을 확인합니다.\n",
    "dist = np.sum(dtm_tfidf, axis=0)\n",
    "pd.DataFrame(dist, columns=cols_tfidf).T.sort_values(by=0).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l61luP9mo1Y2",
    "outputId": "49a5d865-6ee5-4c7d-ff31-217235ea7bfd"
   },
   "outputs": [],
   "source": [
    "# 각 row에서 전체 단어가방에 있는 어휘에서 등장하는 단어에 대한 가중치를 적용한 vector를 확인합니다.\n",
    "# toarray()로 희소 행렬(sparse matrix, 행렬의 값이 대부분 '0'인 행렬)을 NumPy array 배열로 변환하여 값을 확인합니다.\n",
    "pd.DataFrame(dtm_tfidf.toarray(), columns=cols_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMPSs--bo1Y2"
   },
   "source": [
    "## TF-IDF 잠재 디리클레 할당(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bRYL9mho1Y2",
    "outputId": "64d81c7a-e8d0-4697-bef7-f3a4c9415fdf"
   },
   "outputs": [],
   "source": [
    "# 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지를 확인하는 잠재 디리클레 분석(LDA)을 불러옵니다.\n",
    "# n_components에 넣을 하이퍼파라미터 NUM_TOPICS로 주제수를 설정합니다.(기본값=10)\n",
    "# max_iter는 훈련 데이터(epoch라고도 함)에 대한 최대 패스 수입니다.(기본값=10)\n",
    "\n",
    "NUM_TOPICS = 10 \n",
    "LDA_model = LatentDirichletAllocation(n_components=NUM_TOPICS, \n",
    "                                      max_iter=30, \n",
    "                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bRYL9mho1Y2",
    "outputId": "64d81c7a-e8d0-4697-bef7-f3a4c9415fdf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dtm_tfidf 를 LDA_model로 학습시킵니다.\n",
    "LDA_model.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gE9FI9jmo1Y3",
    "outputId": "1f04d309-c867-421e-a948-8e3b1d386fdc"
   },
   "outputs": [],
   "source": [
    "# 토픽 모델링에 이용되는 LDA 모델의 학습 결과를 시각화하는 Python 라이브러리인 pyLDAvis를 불러옵니다.\n",
    "# mds(Multi-Dimensional Scaling)는 데이터 포인트 간의 거리를 보존하면서 차원을 축소하는 기법입니다.\n",
    "# t-SNE(t-Stochastic Neighbor Embedding)은 고차원 데이터를 특히 2, 3차원 등으로 줄여 가시화하는데에 유용합니다.\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(LDA_model, dtm=dtm_tfidf, vectorizer=tfidf, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SfyYdvZo1Y3"
   },
   "source": [
    "## 코사인 유사도\n",
    "* API Document: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AC98n4w5o1Y3"
   },
   "outputs": [],
   "source": [
    "# 등장 빈도에 기반하여, 코사인 유사도 알고리즘 적용해봅니다.\n",
    "# 첫 행의 \"아빠 육아 휴직 장려금\"과 비슷한 데이터 정렬해봅니다.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(dtm_tfidf[0] , dtm_tfidf)\n",
    "result_list = similarity_simple_pair.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJFB-n4Lo1Y3",
    "outputId": "547510bf-d33e-4577-f146-c9c943bbfbc2"
   },
   "outputs": [],
   "source": [
    "# result_list를 \"유사도\" 파생변수로 생성하고 유사도가 높은 순으로 정렬합니다.\n",
    "\n",
    "df[\"유사도\"] = result_list\n",
    "df[[\"분류\", \"제목\", \"유사도\"]].sort_values(by=\"유사도\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXCQLJKGo1Y3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvgfJ7vmo1Y3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
